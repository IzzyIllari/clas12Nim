\section{Trigger System Firmware Development}

After generic Trigger System design was complete and the hardware components entered their production
stage, the firmware development began. Both HLS and VHDL tools were used. Work was performed using data
samples generated by the CLAS12 GEMC/Geant4 package and cosmic data after the hardware components
were installed. This section describes our procedures. Additional development and validation with beam are
described in Section~\ref{sec:validation}.

\subsection{Preparation of Simulated Data Sets}
\label{simulated_data_preparation}

Simulated data sets for Trigger System development were prepared using GEMC (the Geant4-based CLAS12
simulation package~\cite{gemc-ref}). GEMC has a fully realistic CLAS12 geometry description and complete
maps of the magnetic field, and produces digitized results suitable to be converted into the pre-trigger data
format.

Various data sets were generated depending on what was needed for the development of particular trigger
components. For example, fixed-energy, single electron sets were produced for the initial development of the
EC and PCAL components. For these data sets, all detectors positioned upstream of the EC or PCAL were
disabled to make sure the single electron directly hit the EC or PCAL. In this way the cluster-finding algorithm
could be developed and tested in ideal conditions. After that, realistic data samples were produced and the
algorithm was tested again.

\input{DC_gemc_dictionary}

\subsection{Development Using Simulated Data}

The trigger development process consisted of several methods that depended on the nature of the trigger
component. Most Stage~1 components were implemented using the HLS/VIVADO tool, where the firmware was
written using an HLS C++ extension. In that case, it was possible to develop and validate the firmware as part
of the offline reconstruction framework using a standard desktop computer. Usually the offline processing
algorithms were re-written using HLS/C++, with appropriate simplification and structural changes to make it
suitable for the FPGA firmware. Simulated data were used as input, which were processed directly by the
HLS/C++ code and compared with the initial simulation parameters. In addition, the same samples were processed
by the offline reconstruction software and the results were compared with the trigger output. This double-check
method practically guarantees bug-free implementation. There was no single case when the C++ implementation
passed tests on the simulated data and then failed during the final validation stage. The most complicated
Stage~1 components were developed and tested using this method.

Several components of the trigger were written mostly in VHDL and initially no software existed for feeding
GEMC data into the HDL simulations. This was the case for the Stage~1 FT-Cal+FT-Hodo and DC trigger, as well
as the Stage~2 and Stage~3 components of the trigger. These modules relied on standard VHDL test benches
to feed/generate test vectors for evaluating the correctness of the design modules. For example, the FT test
bench generated clusters at each position of the calorimeter and hodoscope to test the channel mapping and
geometry matching. Additional specific test cases verified the FT-Cal+FT-Hodo trigger clustering time
coincidence, cluster multiplicity, and latency to ensure it operated as expected. C/C++ modules were written
that emulated the FT-Cal+FT-Hodo and the DC trigger so the algorithms could work in the same offline
framework as described above for the other Stage~1 components.

\subsection{Development and Validation Using Cosmic Data}

When the hardware components for the CLAS12 detector were constructed and mostly installed, and the first
version of the firmware was ready for testing, all three Trigger System firmware stages were loaded and
development continued for the entire Trigger System using cosmic data. At that point we started to perform
Trigger System validation for some components, while development was continued for others, as described in
the following sections.

\subsubsection{Alternative ``Hit-Based'' Trigger System}

The CLAS12 detector inherited some components from the original CLAS detector (see Ref.~\cite{clas-nim}),
in particular its Trigger System. That system was fed by TDC/discriminator boards and was able to produce
``hit-based'' information only (i.e. based only on the list of channels above threshold). We decided to keep it
for reference purposes as an alternative to the new Trigger System. It was used during CLAS12 cosmic data
detector calibrations and validation of the new Trigger System up to the point when the new Trigger System was
ready. It is still operational and can be used to double-check the main CLAS12 Trigger System if needed.

\subsubsection{Development and Validation of ECAL Special Purpose Trigger with Cosmic Data}

The first detector calibrations employed cosmic data. Here we will describe, as an example, one of the
procedures related to the EC/PCAL calibration needed for correct Trigger System performance. Similar
procedures were executed for all detectors participating in the Trigger System.

The efficiency and spatial uniformity of the cluster finding trigger in the EC/PCAL described in
Section~\ref{sec:ECAL} requires already calibrated calorimeters with pre-determined PMT gain and light
attenuation constants loaded into the VTP/FPGA trigger firmware.  Calibration runs using a special-purpose
MIP trigger were used to obtain these constants. For that purpose a so-called ``pixel trigger'' was developed
and loaded into the Stage~1 firmware along with the main trigger, so it was possible to calibrate the system
using a pixel trigger and then switch to the main one for data taking. This ``pixel trigger'' used a simple
multiplicity condition on 1D cluster size for each U,V,W view to reject undesirable muon trajectories and
select normally incident tracks.  This reduced the trigger and data rate by 95$\%$ and ensured the same
MIP energy was deposited for all possible triple intersections of single strips.

The pixel trigger pipeline executes these steps in parallel, with user configurable parameters in bold:
1) If FADC hit energy $>$ \textbf{EMIN}, make a pulse \textbf{HITWIDTH}*4~ns for that strip. 2) Look
for coincidences of U,V,W pixel strip candidates from step 1. 3) Evaluate multiplicity
\textbf{EVALDELAY}*4~ns clock cycles after the leading edge of a candidate pixel from step 2. 4) Generate
a pixel trigger if the multiplicity requirement is met and we still have a hit on U, V, and W. 

Additional configurable trigger elements were introduced, including a total energy sum threshold \textbf{ESUM}
and a look-up table for triplets of strips that satisfy the geometrical constraint $dU+dV+dW=\textbf{DALITZ}$,
where $d$ is the normalized distance to the hit strip indicated by the arrows in Fig.~\ref{fig:pcal_cosmic_1} and
$\textbf{DALITZ}=2$ for perfect pixel events. The latter test was sometimes necessary to prevent noisy
PMTs from saturating the multiplicity (N=3) trigger condition. Offline analysis showed that about 90\% of
pixel triggers satisfied the Dalitz test (see Fig.~\ref{fig:ec_offline}), while adjacent calorimeter elements that
did not use the trigger had a much smaller pixel fraction.  This suggests the pixel trigger helps to suppress
events that undergo multiple scattering, which would trigger adjacent strips and violate the multiplicity
requirement.

\begin{figure}[!htb]
 	\centering
  	\includegraphics[width=0.95\columnwidth,keepaspectratio]{img/TwoClusters.png}
 	\caption{Examples of clusters from cosmic muon triggers.  The desired trajectory (left) is normally
          incident on the face of the PCAL and satisfies the single pixel multiplicity condition
          (N$_u$=N$_v$=N$_w$=1) in the FPGA pixel trigger. The event at right shows a more vertical trajectory
          rejected by this trigger.}
	\label{fig:pcal_cosmic_1}
\end{figure}

\begin{figure}[!htb]
 	\centering
  	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/PixelFraction.png}
 	\caption{Offline analysis of events that satisfied the pixel trigger in the $EC_{inner}$  calorimeter. The
          left plot shows that 89\% of the $EC_{inner}$ triggers satisfied the pixel test $dU+dV+dW=2$. The right
          plot shows that only 14\% of the $EC_{inner}$ triggers found an $EC_{outer}$ event that satisfied both the
          $N=3$ and pixel test.}
	\label{fig:ec_offline}
\end{figure}

\subsubsection{Development and Validation of DC Component of the Trigger System with Cosmic Data}

Early tests of the Drift Chamber tracking trigger were done using cosmic events triggered from the ECAL. A
small fraction of events had tracks near the target location where the road dictionary was defined, but within
a day enough statistics could be collected to do checks that the tracking trigger was functioning. Offline
reconstruction of events with reconstructed tracks was checked to see if the tracking trigger fired as shown
in Fig.~\ref{fig:dc_cosmic_efficiency}. Any events with tracks that passed through the target, but failed to be
tagged by the tracking trigger were run through simulations to identify the reason. The tests clearly showed
very loose acceptance and motivated tighter kinematic constraints on the dictionary generation that were
eventually done when studies with beam were later performed.

\begin{figure}[!htb]
 	\centering
  	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/dc_cosmic_efficiency.png}
 	\caption{DC negatively charged cosmic tracks rejected (left) and accepted (right) by the tracking trigger
          in plots of reconstructed momentum vs. reconstructed $z$-vertex position. Any track rejected above
          1~GeV with $z$-vertex within $\pm$20~cm would indicate a trigger failure, but the tests showed 100\%
          efficiency.}
	\label{fig:dc_cosmic_efficiency}
\end{figure}

\subsubsection{Development and Validation of Entire Trigger System with Cosmic Data} 

While the Stage~1 trigger components were validated separately from each other during the development
stage, the Stage~2 and Stage~3 components required the entire system to be assembled to perform
validation. Initially those two stages were programmed with simplified algorithms to test signal propagation
and basic Trigger System functionality, and only the timing coincidence between different detectors was
implemented. The development of Stage~2 and Stage~3 continued during cosmic run operations and later with
beam operations, adding geometrical matches between different detectors and increasing the coincidence
logic complexity.

\subsubsection{Trigger System Flexibility and ``Permanent Development'' Mode}

The initial plan was to develop the Trigger System firmware to satisfy all CLAS12 experiments for the entire
duration of CLAS12 operation, meaning high (close to 100\%) trigger efficiency and reasonable purity. As the
power and flexibility of the Trigger System was revealed to the community, additional requirements were
requested to improve the system purity, and to include additional physics processes. As a result, the Stage~2
and Stage~3 components of the Trigger System were under constant development during the first year of
CLAS12 operation, and the firmware was upgraded and the entire system was validated after every change.
After a while, the Trigger System reached the point when relatively small improvements in the trigger purity
could only be achieved with significant effort. At that point the development was declared complete. The nature
of the FPGA-based Trigger System allows almost endless improvements, but such a ``permanent development''
mode is not practical.
