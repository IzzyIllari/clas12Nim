\section{Software}

\subsection{CODA DAQ Software}

The CLAS12 DAQ system is based on the CODA (\cite{coda-ref} system developed at JLab (see Fig.~\ref{fig:coda_diagram}), short for CEBAF Online Data Acquisition. It is a software toolkit of applications and libraries that allows the implementaion of a data acquisition system. The scale of the system can range from a few detector channels in a test stand to tens of thousands of channels in a large detector installation in one of the experimental halls. CODA achieves this scaling through modularity and provides a set of supported hardware components along with complementary software components.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/coda_diagram.png}
	\caption{CODA System Diagram. Every box represents a multi-threaded program running on corresponding controller or server. All communication between programs is done over Ethernet using TCP/IP protocol.}
	\label{fig:coda_diagram}
\end{figure}


\subsection {Run Control}

The CODA DAQ system includes a run control facility consisting of a back-end run control supervisor and a front-end graphical operator display that connects to the supervisor and controls its operation. The supervisor in turn controls operation of the many CODA components that participate in the DAQ configuration. The latter are defined in run configuration files that the operator chooses at startup. The Run Control Graphic User Interface (see Fig.~\ref{fig:runcontrol1}) presents the operator with a choice of possible actions that depend on the current state of the run. The supervisor translates the operator choice into appropriate commands to the individual components. Alternatively, limited communication with the supervisor can be performed via command-line scripts. In addition, the supervisor monitors the health and operation of the CODA components and warns the operator or pauses the run if problems are detected.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/runcontrol1.png}
	\caption{Run Control GUI. Main control and monitoring screen for the CLAS12 DAQ system. The Tabs on the right side allows switching between different control screens, such as the configuration editor, current DAQ components status, and DAQ rates.}
	\label{fig:runcontrol1}
\end{figure}


\input{fe-libs}


\subsection{Readout Controller}

The Readout Controller (ROC, see Fig.~\ref{fig:roc_diagram}) software component is a C program running on the front-end controllers such as the Intel-based VME/VXS crate controllers, VTP trigger boards, or regular Linux servers - essentially any hardware receiving data from the front-end electronics. On DAQ startup, the ROC main program starts three threads. After this it just controls the thread health and communicates with the Run Control process. Three threads (readout, processing, and network) pass data from one to another, communicating via circular buffers. The typical number of buffers in each buffer queue is 8 and the size of every buffer is 4~MB, which defines how long the front-end electronics can be read out before blocking triggers in the case of back-end busy conditions.

The first thread (readout) receives data from the front-end electronics and places it into the first circular buffer. That thread can run in polling mode, which occupies an entire CPU core (or optionally in interrupt mode). The CLAS12 readout primarily employs polling mode, which has adequate performance on multi-core controllers. The second thread (processing) reads data from the first circular buffer and performs all needed data processing. In particular it performs the so-called ``disentangling'' as well as data sanity checks. The results are placed into the second circular buffer. The processing component can create its own worker threads to increase processing power if necessary. The final thread (network) reads data from the second circular buffer and sends it over the network to the Event Builder.

The first and second threads can load user code that is compiled separately and downloaded dynamically at runtime, which allows users to develop experiment-dependent processing without recompiling the enitre ROC application.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/roc_diagram.pdf}
	\caption{Readout Controller (ROC) Diagram. The ROC program typically runs on a 4-core VME Intel Controller. It includes 3 main threads performing  VME readout, data processing, and networking.}
	\label{fig:roc_diagram}
\end{figure}


\subsection{Event Builder}

The Event Builder (EB, see Fig.~\ref{fig:eb_diagram}) is the program that receives the data fragments from all readout controllers and assembles it into events. The building process is based on event number, event type, and timestamp of the data fragments: for each event all three values have to be identical for all data from all Readout Controllers. In case of any differences, the DAQ will be stopped and the error reported.

The Event Builder consists of receiving and building parts. The receiving part contains a set of independent threads, one per readout controller connected to it by TCP protocol. Every thread receives data and places it into an internal buffer. If the buffer becomes full, the thread stops receiving data, effectively propagating the busy condition back to the readout controller. The building part has a number of identical building threads, which take turns by getting data from the receiving part of the internal buffers, building events, and placing them into Event Transfer System.

The total number of threads in the receiving part of the Event Builder in CLAS12 DAQ is currently 118, which therefore represents the number of network connections from the Readout Controllers. Because of this the DAQ has to run on a powerful server, with many CPU cores, large memory, and a high bandwidth network card. CLAS12 is using a Dell R730 server with 32 cores, 64~GByte memory, and 40~Gbit network card. This server is adequate for the present CLAS12 DAQ requirements.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/eb_diagram.pdf}
	\caption{Event Builder (EB) Diagram. The EB program typically runs on a multi-core Linux server. It collects event fragments from the ROCs, builds events, and sends events to the Event Transfer system.}
	\label{fig:eb_diagram}
\end{figure}


\subsection{Event Transfer}

The Event Transfer (ET, see Fig.~\ref{fig:et_diagram}) system provides the user with an efficient method for moving bulk data between processes. The ET was not designed as a messaging system but a shared memory system that can be visualized as data containers moving around a circular railway track. In this metaphor, a producer of data (Event Builder) requests an empty container, fills it with data, tags it with metadata describing the contents, and places it at the start of the track. Stations along the track are assigned algorithms for testing the metadata and selecting the containers of interest. The user has the option of making a station blocking or non-blocking. A container stopped at a blocking station holds up all of the other containers behind it on the track. A data consumer attached to the station processes the data in the container and has the option of either putting the container back on the track, where it proceeds to the next station, or returning the container to the station at the start of the track, effectively discarding the data. In the simple example diagram shown in Fig.~\ref{fig:et_diagram}, a data monitoring consumer attaches to a non-blocking station that samples the data of interest. The Event Recorder attaches to a blocking station and records the content of every container to disk. The container is then returned to the start of the track.

Using these simple concepts, complicated data pathways can be constructed. The ET package supports remote consumers connecting to stations over the network, allowing load sharing between multiple machines. 

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/et_diagram.png}
	\caption{Event Transfer (ET) System Diagram. The ET program typically runs on a multi-core Linux server. It accepts events from the Event Builder and passes them through the set of stations, allowing attached programs to perform data processing. The last station typically has the Event Recorder attached to it.}
	\label{fig:et_diagram}
\end{figure}

\subsection{Event Recorder}

The Event Recorder (ER, see Fig.~\ref{fig:er_diagram}) is the software component that receives data from the ET system and records the data in files onto a disk. The ER can write data to one or several files in parallel (so-called multi-stream mode). If multi-stream mode is used, the event order is preserved, but it requires the ER allocated memory size to be larger than the number of streams multiplied by the individual file size.

The ER structure in multi-stream mode is shown in Fig.~\ref{fig:er_diagram}. The distribution thread receives data from the Event Transfer system, searches for the free writing thread, and grabs its semaphore. It starts filling up the buffer of the thread with data until it becomes full. After that it signals the writing thread to start writing the entire buffer to the specified output file name. The writing thread marks itself ``busy'' and writes data to the file, and after that, it becomes ``free'' again. While the writing process is in progress, the distribution thread grabs another free writing thread and the process repeats. The writing performance of the ER can be increased by increasing the number of writing threads.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/er_diagram.pdf}
	\caption{Event Recorder (ER) Diagram. The ER program typically runs on a multi-core Linux server. It receives events from the Event Transfer system and writes them to the disk. It is possible to write several files in parallel to increase writing performance; event order is preserved in a process.}
	\label{fig:er_diagram}
\end{figure}


\subsection{Messaging System (ActiveMQ)}

The messaging system in the CLAS12 DAQ is based on the ActiveMQ library and its C++ extension. Two ActiveMQ servers are used to route all communiations. The number of connections to ActiveMQ is several hundred, and the number of messages sent every second is several tens of thousands, with data volumes of a few tens of MBytes per second. The messaging system is used in particular to monitor and control the DAQ components, in addition to the Run Control process.


\subsection{Runtime Database (RCDB)}

A JLab-designed MYSQL-based Runtime Database (RCDB) is used to store the run parameters and statistics. It has a web interface (see Fig.\ref{fig:rcdb}) and provides an interface to various languages including C++ and Java.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/rcdb.png}
	\caption{Runtime Database Web Browser example that allows users to search for run(s) with certain parameters.}
	\label{fig:rcdb}
\end{figure}

\subsection{Online Data Monitoring}

The CLAS12 online data monitoring is the set of programs attached to the Event Transfer system that processes data in real time. One of such program's outputs is shown in Fig.\ref{fig:online_monitor}. The Monitoring system allows shift takers to insert histograms into the electronic logbook. Every CLAS12 detector has a designated set of histograms that is closely monitored by shift takers and detector experts to ensure data quality.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/online_monitor.png}
	\caption{Online monitoring example. Only a small portion of the histograms is shown. The full set includes hundreds of histograms used by shift takers to monitor various detectors and subsystems.}
	\label{fig:online_monitor}
\end{figure}

\subsection{Electronic Logbook}

The CLAS12 shift personal use the JLab Electronic Logbook system \cite{logbook-ref}. This web-based system provides an interface to browse, search, and create electronic logbook entries. The system has been built using the Drupal content management system and utilizes a MySQL back-end database to store its entries. The system has been designed to consolidate and replace the functionality of the multiple electronic logbook systems that existed at Jefferson Lab between 1996 and 2012 .

\subsection{ROOT for DAQ (FT)}

\input{AndreaDAQ}


\subsection{CLAS Event Display}

The CLAS12 Event Display (CED) is a full-function graphical application that displays online (and offline) events using various representations of CLAS12 called views. The views are independent windows that the user can pan, zoom, scroll, etc. Some of the views are geometrically faithful, and some are designed for maximal information content as opposed to realism. The primary purpose and utility of CED, when used online as part of the DAQ system, is for additional data monitoring. While running, CED will display an event from the live stream at a selectable rate, typically one every two seconds. A quick glance at CED will confirm, for example, that there are data in the drift chambers that appear to form tracks. In this way it serves as an early warning of problems with detectors and/or the data stream. It is also possible to operate CED in a mode where it creates graphical histograms or occupancy overlays. A typical view from CED is shown in Fig.~\ref{fig:ced}.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\columnwidth,keepaspectratio]{img/ced.png}
	\caption{CED Event Example that includes a schematic view of all detectors with a realistic geometry.}
	\label{fig:ced}
\end{figure}
