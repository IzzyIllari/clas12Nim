\section{Data Processing}

\subsection{Data Processing Workflow}
The raw data is currently first preprocessed.  This is an I/O-heavy, single-threaded process and involves extracting hits from waveforms, translating data-acquisition/hardware nomenclature to physical detector objects, performing special analyses dependent on serial event access, and converting from the input EVIO format to the HIPO data format.  This phase includes registering beam helicity state changes and special scaler events, and populating their results into tag-1 HIPO events to faciliate downstream analysis.  The result is a factor of $\sim$5 reduction in size and a file format optimized for I/O.

The second stage is a cpu-heavy reconstruction phase, including all the tracking, clustering, calorimetry, time-of-flight, and event building in the previous sections.  It runs multi-threaded in the CLARA framework and can be configured to output various data schema depending on the purpose, e.g. the DST structures of \ref{sec:dsts} during full-scale processing, or larger, special-purpose banks during preliminary calibrations phases.

The final stage is I/O-heavy analysis trains that perform event skimming and accommodate corrections and common analysis plugins.  It splits the data into multiple output files based on different event selections, each optimized for a group of physics analyses.  This stage is designed to be be run repeatedly as selction criteria and physics analyses mature.

\subsection{DSTs}\label{sec:dsts}
The final data output is provided by the Event Builder in the form of data summary tapes (DST), a standardized selection of HIPO banks for physics analysis.  These include
\begin{itemize}
    \item global event information, e.g. run number and event timestamp, integrated beam charge, beam helicity state, event start time
    \item particle information, e.g. momentum four-vector and vertex position, particle type and identification quality, and status words
    \item high-level detector response information associated with each particle, e.g. detector identifier, response position and time, and track trajectory in each detector layer
\end{itemize}
The DST banks are organized such that the large, detector information can easily be dropped to leave only the data essential for a high-level physics analysis, without leaving dangling references or unecessary information.

\subsection{Computing Resources}
Reconstruction of all CLAS12 data is performed on Jefferson Lab's batch computing system\cite{jlab-batch-farm}.  It currently consists of about 400 compute nodes of various flavors, with a total of about 21,000 available jobs slots and half as many cores.  The input raw data and analyzed output data are stored on JLab's tape silo \cite{jlab-tape-silo}, which provides sufficient cold storage for all of JLab's activities.  Data for physics analysis is also stored live on JLab's Lustre filesystems \cite{jlab-lustre}, which is currently about 2 PB but being increased to almost 7 PB in the near future.  Analysis of reconstructed data is performed on the JLab batch and interactive farm nodes, and also transported to other institutions for final physics analysis.
