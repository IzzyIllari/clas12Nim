%
%---Beginning of Document---
%
% Use 1). latex recon-nim.tex, 2). dvipdfm recon-nim
%
%\RequirePackage{lineno}
\documentclass{elsart}
\usepackage[dvips]{color,graphics}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb,amsmath}
%\setcounter{footnote}{0}
\setcounter{tocdepth}{5}
%
\usepackage{lineno} 
\linenumbers

\begin{document}
\begin{frontmatter}

\title{CLAS12 Event Reconstruction}

\author[JLab]{V. Ziegler\thanksref{corresponding}},
\author[JLab]{N. Baltzell},
\author[Saclay]{F. Boss\`u},
\author[JLab]{D.S. Carman},
\author[IPNO]{P. Chatagnon},
\author[INFN]{R. De Vita},
\author[Saclay]{M. Defurne},
\author[JLab]{G. Gavalian},
\author[JLab]{V. Gyurjyan},
\author[JLab]{N. Markov},
\author[JLab]{M.D. Mestayer}, 
\author[IPNO]{S. Niccolai}, and
\author[Glasgow]{D. Sokhan}

\address[JLab]{Thomas Jefferson National Accelerator Facility, Newport News, VA 23606, USA}
\address[INFN]{INFN, Sezione di Genova, 16146 Genova, Italy}
\address[Saclay]{CEA-Saclay, Univ. Paris-Sud, Universit\'e Paris-Saclay, Gif-sur-Yvettes, France}
\address[IPNO]{Institut de Physique Nucl\'eaire, CNRS-IN2P3, Univ. Paris-Sud, Universit\'e Paris-Saclay,
  91406 Orsay Cedex, France}
\address[Glasgow]{University of Glasgow, Glasgow G12 8QQ, United Kingdom}
\thanks[corresponding]{Corresponding author. Address: 12000 Jefferson Ave., Newport News, VA; 
e-mail: carman@jlab.org.}

\date{\today}

%\maketitle

\begin{abstract}
  We describe offline event reconstruction for the Compact Large Acceptance Spectrometer at 12 GeV.
  The present an overview of the offline reconstruction framework and software tools.  
  We describe the algorithms developed for the individual detector subsystems and the overall scheme for the
  particle identification.  We present the data processing scheme and code management procedures.
\end{abstract}

\end{frontmatter}

PACS:29.40.Mc \\
Keywords: CLAS12, event reconstruction
\newpage

\newpage
\tableofcontents

\vfil
\eject

\section{Reconstruction Framework and Tools}

Software in nuclear physics tends to grow in complexity over time. Because of the tight coupling between
the internal components, it becomes hard to maintain and add new features over time. Modifying a feature
in one class often involves changes in several other classes, thereby increasing the needed development
time and effort. What starts small and simple, inevitably grows into a complicated scheme of interconnected
algorithms. At some point, to prevent further growth in complexity, the software has to be shielded from
contributors, preventing essential software evolution and optimization. The only way to bypass this highly
guarded system is software fragmentation, making the physics data processing validation process quite
arduous.

\subsection{CLARA}

We have developed a reactive, data-stream processing framework called CLARA
\cite{clara-2011,clara-service,framework,clara-2016} that addresses these issues by simply deploying a true
“divide and conquer” principle to the software application, and insures continuous evolution and contribution of
software applications. Nuclear physics data processing applications have a very long lifetime, and the ability to
upgrade technologies is therefore essential. Hence, data processing applications must be organized in a way 
that easily permits the discarding of aged software components and the inclusion of new ones without having to
redesign entire software packages at each change. The addition of new modules and removal of unsatisfactory
ones is a natural process of a software application evolution over time.  Software evolution and diversification
(e.g. using heterogeneous hardware structures, such as FPGAs, GPGPUs, etc.) is important and results in more
efficient and robust data processing applications.

Software applications designed within the CLAS12 CLARA data processing framework differ from ones developed adopting 
a traditional approach in two major ways. First, software applications are composed of interlocking software building blocks
(bricks) called micro-services. Micro-services are linked together by data-stream pipes.  The technology
(e.g. high-level programming language, hardware deployment details) as well as the algorithmic solutions used to process
data (see Fig.~\ref{clara-overview}) are encapsulated. A micro-service has inputs, processes data and produces output data.
A micro-service reacts on a streaming data quantum in its input, processes it, and 
passes processed data quantum to the next micro-service in the data-flow path. The second and critical difference is 
that CLAS12 data processing applications execute according to the rules of data-flow instead of a more traditional 
programming approach, where sequential series of instructions (lines of code) are written to perform a required algorithm. 
In this respect CLAS12 data processing framework application design promotes a data-flow as the main concept
behind any data processing algorithm.   The flow of data between micro-services in the application determines
the execution order of micro-services within the application. These differences may seem minor at first, but
the impact is revolutionary because it allows the data paths between application building blocks to be the
application designer’s main focus. As a result, CLAS12 data processing application is more robust and agile, since
application building blocks can be improved individually and be replaced.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\vspace{5.0cm}
\begin{picture}(50,50) 
\put(45,-5)
{\hbox{\includegraphics[width=1.0\textwidth,natwidth=610,natheight=642]{pics/clara-overview.pdf}}}
\end{picture} 
\caption{Overview of the CLAS12 CLARA data processing framework that links the detector subsystem
event reconstruction applications to the data processing and data quality assurance monitoring applications
to the physics analysis applications.}
\label{clara-overview}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Common Tools}
\label{common-tools}

CLAS12 consists of many detectors for particle tracking and identification. At the start of this project software
group decided to develop common software packages that can be shared between different detector reconstruction
packages to avoid code replication and aid in software maintainability.

The common software was divided into packages, each having a specific purpose and functionality. Below we discuss
the main packages used in the reconstruction software.

\subsubsection{Geometry}

Due to the complexity of the geometry of CLAS12 detector subsystems, an interface was developed 
to provide classes and geometry software tools that are used to describe the geometry of all subsystem in an unified way.

A library of primitives was developed to provide geometrical objects needed to represent by all geometry detector subsystem
and for geometrical object transformations.  These primitives include lines, planes and various shapes (e.g. 
cubes, trapezoids, etc.).  They provide the functionality needed
to represent the detector subsystem components in the reconstruction frames, to take into account geometrical distortions
and to translate and rotate detector components which is particularly relevant for alignment purposes. 
Further more they provide methods to track particles through volumes, and return informations used to determine 
track trajectories,
such as line to surface intersections, ray tracing through objects, distance of closest approach to a line or surface.

The CLAS12 geometry library insures consistency between the geometry implementation used in CLAS12 simulation, reconstruction and 
event visualization packages. It is initialized through database, and provides output for GEANT4 simulations package
(GEMC), is used in reconstruction application and in the CLAS12 Event Display (CED) .

\subsubsection{Databases}

The Calibration Constant Data Base (CCDB) software package was developed at Jefferson Lab for the GlueX experiment 
in HallD.  The CLAS12 reconstruction packages use the CCDB application programming interface to create and access 
tables that contain detector geometry and calibration constants, as well as maps used for decoding raw data.
CCDB provides the functionality for storing and accessing 
structured tables in MySQL-based and SQLite portable databases. 

The CCDB package creates
tables that link constants to specific runs (using timestamps) and store different variations of constants depending on run
conditions. CLAS12 software tools employ an Application Programming Interface (API), that parses database tables and creates
structured maps of constants stored in  memory by detector sector, layer and component. This allows fast retrieving of the 
constants.

The CLAS12 database access tools have been written to avoid bottlenecks that might result from multiple multi-threaded 
services accessing the database to retrieve constants.  An interface has been designed to fetch the constants 
on demand and cache them for further requests. In this approach each service will request the
 constants it requires on one thread and each subsequent request by a new thread will be provided by the cached values.

\subsubsection{Plotting}

For ease of integration with the reconstruction software tools and packages, the plotting tools used for data 
analysis, calibration and monitoring was written in the JAVA programming language. 

The plotting software (called groot) developed at Jefferson Lab for CLAS12 is tailored to have a programming 
interface similar to ROOT
(CERN data analysis package) and provides the functionality necessary for 
fitting using the JAVA-based MINUIT library available from the JHEP repositories.

\subsubsection{Analysis Tools}

Analysis Tools are under construction.

\section{Data Formats}

\subsection{EVIO}

\subsection{HIPO}

The CLAS12 data format was designed to provide a flexible data container structure,
 with features that minimize disk
access for the most common tasks performed in data analysis. 
The High Performance Output (HiPO)
format developed for CLAS12 was designed to provide data compression, using LZ4 (fastest compression algorithm available), 
and random access. 

HiPO stores data in separate records (with adjustable size), with tags associated with each record. Each record is
compressed and a pointer to the record is kept in the file's index table. This feature allows separating events during
reconstruction based on the content of the event, such as number of reconstructed particles. Users can read portions
of the file depending on the final states to be analyzed.  The meta-data of the file, describing detector and
beam conditions are common for all analysis.

A C++ interface is also provided with ROOT wrappers, allowing users to read HiPO files and analyze them in ROOT
environment.

\section{Monitoring and Calibration Suites}

\subsection{Framework}

A calibration framework was developed to implement visualization software tools needed for all detector 
systems. Standard views were developed using the JAVA Swing application to visualize detector components and to provide callback
mechanisms necessary to display detector-component specific information.  These software tools provide 
functionality for data fitting, plotting and displaying using a graphical user interface environment. 

The calibration framework makes use of the other CLAS12 libraries 
(the geometry and plotting packages, as well as database utilities) and provides a uniform Graphical User
Interface (GUI) for all calibration applications. The framework provides a data processing interface  
and a calibration constant database interface used
for on- and off-line data analysis.   

A common data streaming interface is implemented with software level abstraction that allows calibration and monitoring
code to run on variety of supported data formats used in CLAS12.

\subsection{Data Monitoring}

\subsection{Calibration Suites}

The software programs used for the CLAS12 detector subsystem energy and time calibrations are
Java-based suites that employ the framework discussed in Section~\ref{common-tools}.
The software tools provided by the framework facilitate the development of 
detector-specific suites. Fig.~\ref{suites} shows representative views of CLAS12 subsystem calibration suites.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\vspace{4.5cm}
\begin{picture}(50,50) 
\put(55,-5)
{\hbox{\includegraphics[width=1.0\textwidth,natwidth=610,natheight=642]{pics/suites.png}}}
\end{picture} 
\caption{Representative subsystem calibration GUIs for ECAL~\cite{ecal-nim} (upper left),
  DC~\cite{dc-nim} (upper right), FTOF~\cite{ftof-nim} (lower left),  and FT~\cite{ft-nim} (lower right).}
\label{suites}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The calibration applications take as input raw or reconstructed data files 
(from either beam data or Monte Carlo simulations) in
either EVIO or HIPO data formats.  They display the various quantities and histograms relevant to the 
extraction of the calibration constants.  Each calibration suite performs fits to determine the detector subsystem 
the calibration constants.  The calibration analysis parameters are saved into ASCII files 
with the same structure as the tables defined in CCDB.

\section{Event Reconstruction}

\subsection{Introduction}

\subsection{Tracking Overview}

The event reconstruction software has been designed and developed within the ClaRA framework. The
reconstruction program must reconstruct, on an event-by-event basis, the raw data coming from either
simulation or the detectors to provide physics analysis output such as track parameters and particle
identification. Charged particle tracking is separated into the reconstruction of tracks in the central
(Silicon and Micromegas Vertex Trackers) and forward (Forward Micromegas Tracker and Drift
Chambers) detectors. The forward region covers the angular range from $5^\circ$ to $40^\circ$, while
the Central Detector covers approximately $40^\circ$ to $135^\circ$. In the central region a 5~T
solenoidal magnetic field bends charged tracks into helices, while forward-going tracks are bent by a
$\sim$2~T toroidal magnetic field. For both systems, track reconstruction comprises algorithms for
pattern recognition and track fitting. Hit objects, corresponding to the passage of a particle through a
particular detector component, require the transformation of an electronic signal into a location of the
track's position in the detector sub-system geometry. A hit is thus a geometric object, for example, a
line segment. These objects then form the input to the pattern recognition algorithms. This first step
involves the identification of clusters of hits and the determination of the spatial coordinates and
corresponding uncertainties for hits and clusters of hits. At the pattern recognition stage, hits that are
consistent with belonging to a trajectory (i.e. track) are identified. This set of hits is then fit to the
expected trajectory with their uncertainties, incorporating the knowledge of the detector material and
the detailed magnetic field map.

\subsubsection{Forward Tracking}

The Drift Chamber (DC) wire hit information is given by the wire geometrical location and the drift time
to the wire. Track-dependent corrections to the hit, such as left-right ambiguity and time-walk must then
be performed. Pattern recognition for the DC is done to first order on a ``hit-based'' basis. In hit-based
tracking, a hit is defined as a wire with a recorded signal.  No timing information is incorporated at the
preliminary stage of the reconstruction.  Uncorrelated hit noise in the DC are identified by a Simple Noise
Removal (SNR) algorithm and rejected. SNR stores all the data for a drift chamber layer (112 sense wires)
bitwise in an extended 128-bit word, with "set" bits corresponding to hits. The algorithm is configured
through parameters specifying the maximum tilt of a track segment and the number of missing layers
allowed in the formation of a segment. Using bitwise operations on the extended words, the algorithm
essentially operates as a parallel processor on all 112 sense wires in layer. This parallelism precludes the
need of a wire for-loop, enabling the algorithm to run in a negligible fraction of the total time for
reconstruction. More to the point, SNR actually saves time by reducing the combinations that must be
explored in the pattern-recognition phase of the ensuing track-finding. The hits remaining after the SNR
algorithm are grouped into clusters. 

Subsequent ``noise rejection'' algorithms are then applied to the clusters. These include the removal of the
interior hits from horizontal ``strings'' of hits along a layer~\footnote{The effectiveness of this cut follows
  from the observation that high-momentum tracks from hadrons typically cross the superlayers at a large angle,
  while ``curlers'' from low-momentum background follow curling trajectories with a significant part of the
  pattern being along layers.}, overlapping segments resolving, attached noise hits (resolved mostly using timing
information). Overlapping segments are produced when the trajectories of tracks cross each other 
or when the tracks are almost parallel and very close to each other in a given region. 
A hough transform is employed to find hits on a line in the cluster which allows splitting the cluster into 
segments.  The resulting trimmed clusters are then fit to a straight-line hypothesis, and those hits with
acceptable residuals are kept and identified collectively as a ``track segment''. 

Fits to the segments with a linear function are a preliminary step to estimating
a track trajectory. Track parameters are estimated in the local coordinate system of the chamber sector from
this trajectory. 

Using the wire direction in a given superlayer along with the line fit to a segment in that superlayer, a plane can be constructed. 
Thus pairs of segments in
neighboring superlayers within one chamber (with superlayers of $\pm$6$^\circ$ stereo angle) represent the
intersection of two planes, that is a line whose coordinates are evaluated midway between the two
superlayers, and is a 6-dimensional object (x,y,z and 3 angles) which we call a ``cross''. 
A segment slope coincidence algorithm is used to match neighboring segments in a region.  Selection cuts are subsequently applied on the 
reconstructed cross to ensure that it is within the fiducial sector volume within resolution. 

An additional  pattern reconstruction algorithm  matches segments within even and odd superlayers, respectively, to form
a track candidate where an entire superlayer can be missing.  The matching algorithm returns an estimates of where the missing
superlayers hits should be and forms a pseudo-segment from the wire locations corresponding to these hits.  Subsequently 
a pseudo-cross is formed using the pseudo-segment and the neighboring reconstructed segment in that region. 

The first stage of
pattern-recognition consists of finding a track candidate, from a set of 3 crosses (one each in R1, R2 and R3) that
are fit to a parabolic functional form to give us a ``track candidate''.  
Using the parameters of the parabolic function between the first and the third cross and obtaining the magnetic field 
intensity at each step along this trajectory we obtain an estimate for $\int B dl$. 
From the local angles of the crosses in the $x-z$ plane for R1 ($\theta_1$) and R3 ($\theta_3$) we estimate the track momentum ($p$)
and the particle's charge ($q$),
as follows:
$$\frac{q}{p} = \frac{\theta_3 - \theta_1}{0.3\int{B dl}},$$ where the angles are in radians, the magnetic field 
intensity ($B$) is in Tesla, and the pathlength ($dl$) in cm.  
 The cross position and angles in region 1 together with the 
momentum and the charge provide all the necessary information to define the track parameters at a given location in the detector, 
and therefore to start track fitting.

The output of the pattern recognition
is a seed with initial parameters used to start the track propagation from one measurement site to the next in the fit.
Track fitting uses a Kalman Filter method with a 5-parameter track representation (x, y, tx, ty, q/p), defined
in a local coordinate system with the z-axis perpendicular to the chambers wire planes; where $tx=px$/$pz$,
$ty=py$/$pz$, in that frame.  
In that frame the state vector (representing the track parameters) and the measurement are defined at each layer 
at which there is a hit on track. Hence as in ref~\cite{spiri} we can express the equations of motion of the track in the torus field
and the propagation of the state vector covariance matrix as derivatives with respect to $z$. 
These equations  are solved using a Runge-Kutta 4 numerical method.  
The measurements used in the fit correspond to the drift distances to the wires
taking into account the left/right position of the track with respect to the wire. Time corrections are applied
after ``hit-based'' tracking to account for the event start time, cable delays, propagation times along the
wires, event flight times to the hit location along the wires, beta-dependent corrections, and the effect of
the magnetic field on the cell isochrones which modify the drift times.  

After the times are corrected, the drift distance is computed using a tabulated distance-to-times
multi-dimentional arrays.  The drift distances are computed using a multi-dimentional interpolation method
using the segment local angle, the value of the magnetic field at the location of the hit and the corrected
times.  The Kalman fit is redone at ``time-based'' level using hits with corrected times and computed drift
distances. 

A graphical representation of tracks in the CLAS Event Display is shown in figure~\ref{fig:dcTracks}.  The
display shows a longitudinal view of the chambers trapezoidal shapes. The field intensity is represented by a
color gradiant. The drift chamber hits correspond to the maroon shapes.  The bottom track goes through a
characteristic noise pattern in Region 1 (R1). Only the first superlayer in R1 is used to fit this track.  The
tracks hit the Forward Time-Of-Flight system and the Calorimeters.  The top track leaves a hit in the HTCC
and is identified as an electron. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\vspace{4.5cm}
\begin{picture}(50,50) 
\put(120,-5)
{\hbox{\includegraphics[width=0.6\textwidth,natwidth=610,natheight=642]{pics/dcTracks.png}}}
\end{picture} 
\caption{Event display of charged particle tracks in the Drift Chambers.}
\label{fig:dcTracks}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Central Tracking}

Tracks whose angle with the beam direction is between $40^\circ$ and $135^\circ$ are reconstructed by the so called 
Central Vertex Tracker (CVT). The CVT consists of twelve cylindrical layers of tracking detectors, numeroted from 1 for 
the most inner layer to 12 for the most outer layer. The subset of tracking detectors forming layer 1 to 6 are silicon 
strip detectors and will be referred as the Silicon Vertex Tracker (SVT) described in~\cite{svt-nim}. The layers from 7 
to 12 are made of micromegas detectors and will be referred as the Barrel Micromegas Tracker (BMT) described 
in~\cite{mm-nim}. The entire CVT surrounds the target and sits in a solenoid magnet able to produce a 5T-magnetic 
field.  

The revolution axis of the CVT coincides with the ideal beam axis which defines the z-axis of the CVT frame. The y-axis 
points upward in the laboratory frame and the x-axis is defined such as the unit vectors of each axis form an direct 
orthogonal basis. The origin of the CVT frame matches the center of the experimental Hall B. 

The first step of the tracking algorithm is the formation of clusters from the raw hits. The SVT raw 3-bit ADC values 
are transformed into deposited charge by randomly sampling a Landau distribution fitted on test-bench data. A cluster 
is a collection of contiguous hit strips. Its centroid is either given by a spatial information (a z-coordinate for 
BMTC, xy-coordinates for BMTZ) or an averaged strip number for SVT. This centroid is derived by performing an average of 
the relevant strip information, weighted by the maximum of the ADC pulse for micromegas strips or the equivalent 
deposited charge for the SVT.
The time information associated to each hit is not used at the moment.

Before feeding all the CVT clusters to a pattern recognition algorithm, spatial coordinates must be associated to the 
SVT clusters. As described in~\cite{svt-nim}, SVT layers are mechanically paired and consequently form three regions. 
The readout strips of the inner and outer layer of each region make a $3^\circ$-stereo angle. By associating one 
cluster of the inner layer with one cluster of the outer layer, and by assuming that an infinite momentum track crossed 
perpendicularly the two layers, a preliminary candidate of the xyz-coordinates of the particle between the two layers 
is derived for this cluster pair. This pairing is performed over all clusters of inner layer with all clusters of the 
outer layer. Pairs whose xyz-coordinates lives outside of the SVT sensor are automatically removed from the list of 
xyz-candidate list. If one of the two layer of a region has not seen the particle, then the information of the active 
layer is simply ignored for the reamining of the reconstruction process.

The trajectory of a charged particle in a solenoidal magnetic field is an helix. Because BMT detectors offers either 
xy- or z-coordinates but never both, the pattern recognition cannot be performed in 3 dimensions. For particles of 
large enough momentum ($p_\perp \gtrsim 0.25 \text{GeV}/c$ for a 5T-solenoidal magnetic field), the xy-projection of an 
helix is a circle, and the $rz$-projection is a straight line (where $r = \sqrt{x^2 + y^2}$). Therefore, a first 
pattern recongition algorithm is run in xy-plane to look for circles and then a second pattern recognition algorithm is 
run in rz-plane to search for straight lines.   

The two pattern recognition algorithms are a modified version of the cellular automaton (CA) algorithm developed by the 
Hera-B collaboration~\cite{CA-HeraB}. Here, the elementary cell of the CA is defined as a segment that connects two 
2D-points.
In the $(x,y)$ plane, cells are formed with SVT and BMTZ xy-information. Two xy-clusters form a cell if the angular 
distance between them is lower than a threshold. This threshold has been derived by maximizing the reconstruction 
efficiency on a single track Monte-Carlo simulation with background extracted from the data. Two clusters cannot form a 
cell if they are separated by more than one layer. Finally,The CA is run sector-by-sector in the BMT and, as a 
consequence, a cell cannot be formed with two clusters living in different BMT sectors. 
The subsequent step is the neighbor finding. A cell ``a'' is neighbor of a cell ``b'' if they share one cluster and if
the layer numbers in ``b'' are higher than the ones in ``a''. Tuned on single-track Monte-Carlo simulation without 
background, cuts on the dot product between the cell directions are applied  as neighbor-forming criteria.
Now that the neighborhood of a cell is defined, the CA is evolved over $N$-evolution stage: For evolution stage $n$, 
the state of all cells is updated according to $S_n = max(S_{n-1}^j) + 1$, where $S_{n-1}^j$ is the state of the 
j$^{th}$-neighbor of the considered cell at evolution time $n-1$. Therefore, at evolution stage $N$, the cells with the 
highest state are outer than the cells with a smalle state.
\color{red} \textbf{Caption of a figure to be produced to explain CA evolution} \color{black}

Track candidates in the $(x,y)$ plane are then formed starting from the highest state cells and following the neighbor 
chain with $\Delta S = 1$. In case of multiple neighboring cells with the same state, the one that has the lower dot 
product with the original cell is chosen.

Due to the poor $z$-resolution of the preliminary SVT 3D-points, the search for candidates in the $(z,r)$ plane is 
performed by only using 
the z-clusters of BMT. The CA algorithm trivially returns the track segments of two or three BMTC clusters.
Due to the orthogonality of the BMTC and BMTZ, all the $(z,r)$ segments of a BMT sector are combined with the $(x,y)$ 
candidates in the same sector. A line is fitted on the BMTC hits and its intersections with the three SVT regions are 
computed. If the distance between the expected intersection and the preliminary 3D-point in the SVT region is greater 
than two millimeters, then the two SVT clusters forming this preliminary point are removed from the track candidate.

Each track candidate is then passed to a Kalman filter. The state vector to describe an helix is formed by five 
parameters $(\varphi_0, d_0, \kappa, z_0, tan(\theta_{dip}))$, where:
\begin{itemize}
\item $d_0$ is the $(x,y)$ distance of closest approach to the CVT revolution axis,
  \item $\varphi_0 = atan(p_y/p_x)$ at closest approach the the CVT revolution axis, 
  \item $\kappa=Q/p_\perp$ and $Q$ is the electric charge and $p_\perp=\sqrt{p_x^2+p_y^2}$,
  \item $z_0$ is the distance along the $z$ axis to the CVT center,
  \item and $\theta_{dip}$ is the polar angle between the track and the xy-plane.
\end{itemize}

To initialize the Kalman Filter, a first estimate of these parameters are obtained from:
\begin{itemize}
\item a circle fit in the xy-plane with SVT preliminary 3D-points and BMT xy-clusters for $d_0$, $\varphi_0$ and 
$\kappa$. To improve the initialization of the fit, the point (0,0) is included in the fit with an accuracy of 100~$\mu 
m$.
\item a line fit in rz-plane using only z-clusters of Micromegas to initialize $z_0$ and $\theta_{dip}$.
\end{itemize}
The covariance matrix of the two fits are merged into a 5$\times$5 matrix to initialize the covariance matrix for the 
Kalman filter. Following transport equations in~\ref{ILC-Tracking}, the state vector is propagated from the CVT 
revolution axis to the most outer layer of CVT, filtering at each measurement composing the track candidate. Once the 
last measurement is reached, state vector and covariance matrix are brought back to the CVT revolution axis as they are 
and the transport/filtering process is re-run. A maximum of five iterations is performed to make sure of the 
convergence of the filtering process. 

\subsubsection{Tracking Efficiency and Resolutions}

\subsection{Electromagnetic Calorimeters}
\section{Reconstruction}

The FEC reconstruction service provides a fast and efficient algorithm for grouping scintillator {\it strips} with
{\it hits} into multiple {\it peaks} and {\it clusters} within a single sector for each of the FEC modules PCAL, ECIN,
and ECOU, while leaving cluster matching and particle ID to the event builder service.  Within the FEC service, these
various elements exist as objects with methods, structures and data members designed for calibration, pattern
recognition, diagnostics and serial output.  For example the service applies run dependent calibration corrections for
conversion of raw FADC and TDC digitized data to energy and time, and also provides formatted output banks used by
external services.  Also energy thresholds and cluster identification criteria can be configured to optimize reconstruction
efficiency, suppress backgrounds and avoid false or duplicate clusters arising from fluctuations at the fringes of EM
showers. 

The cluster finding algorithm makes use of the unique geometry and stereo readout features of the FEC. As discussed
earlier, each triangular scintillator layer in the FEC lead:scintillator sandwich is transversely divided into strips, with the
shortest strip at the corners. The slice direction rotates by $\approx~120^{\circ}$ for each successive layer, providing
three {\it views} labeled $U$, $V$, and $W$.  For each strip within a view, layers are optically ganged together into a
stack.  Individual PMT readout of each PCAL, ECIN, and ECOU stack provides a pulse proportional to the summed energy
deposited in the stack.

The algorithm begins by finding collections of contiguous strips having signals above a user-defined threshold for each of
the three views. These groupings are called {\it peaks} and their member strips are referred to as {\it hits}.  Peak objects
may be further subdivided based on the hit energy profile of the groupings.  Each peak object is associated with the one or
more stacks of strips which belong to it, and the three-dimensional geometry of each stack is stored along with the peak
data. The service uses this geometry data to determine which collection of peaks belong to $\it{clusters}$. 

\subsection {Cluster Position}

The criterion for a cluster requires the spatial intersection of three peaks, one from each of the $U$, $V$, and $W$ views.
Candidate peaks for a cluster search are based on a user-defined threshold for the summed peak raw energy.  Each peak is
represented geometrically as a directed line segment determined by the energy weighted average of the mid-lines of each
member strip.   The degree of intersection of each $U$, $V$, $W$ peak triplet is determined by calculating the line of
closest distance between a $U$ and $V$ peakline, followed by the line of closest distance between the midpoint of the $UV$
line and the $W$ peakline.  A user-defined cut on this final $UV$-$W$ distance identifies the cluster, and the midpoint of
the $UV$-$W$ line defines the $(x,y,z)$ coordinates of the cluster.  

\subsection {Cluster Energy}

Once the cluster is localized, the path from the cluster position to the PMT readout end is calculated for each $U$, $V$,
$W$ peakline and the peak energies are corrected for scintillator light attenuation.  For isolated clusters the cluster energy
is then defined as the sum of the corrected energy from each of the $U$, $V$, and $W$ peaks which define the cluster.  

More complicated scenarios arise from the triangular geometry of the FEC hodoscope, which creates the possibility of a
single peak in the $U$, $V$, or $W$ view sharing the summed energy from two or more clusters.  For these cases the energy
in each cluster which shares that peak is assumed to be proportional to the relative partial energies of the multiple clusters as
measured in the other views.  For example, if there are two clusters, both of which share the same $U$ peak, the summed
energy $V+W$ is determined for each of the clusters, and the ratio of these summed energies determines how much of the
$U$ peak energy is assigned to each of the two clusters.  

Finally the clusters to be reported to external services are selected with a user-defined energy cut, and these clusters
are sorted according to energy. Typical software thresholds applied at the strip, peak and cluster level are 1, 3, and 10~MeV,
respectively. 

\subsection {Cluster Time}

Once the cluster is localized, the path from the cluster position to the PMT readout end is calculated for each $U$, $V$,
$W$ peakline and the peak timing is corrected for propagation delay of the light, using the effective velocity of light
determined for each scintillator from the calibration procedure.  For isolated clusters the cluster timing is then taken from
the $U$, $V$, or $W$ peak with the largest uncorrected raw ADC value.  The minimizes the effect on the timing resolution
from both the time-walk correction and the photoelectron statistical fluctuations.

\subsection{Cherenkov Counters}

\subsubsection{High Threshold Cherenkov Counter}

The goal of the HTCC reconstruction algorithm is to calculate the signal strength, time, and position from the
raw ADC signals coming from the PMTS through the corresponding FADC board. The algorithm takes into account
the properties of the HTCC geometry, namely,  the possibility for the signal from the single track to split into
four mirror and hence produce four separated signals (or hits). The final signal reconstruction is done in three
steps: decoding, hit reconstruction, and cluster reconstruction.

In the decoding stage, the signal is converted from the hardware notation (crate, slot, channel) into the CLAS12
notation (sector, layer, component).  For each signal, strength ($nphe_{hi}$) and timing ($T_{hi}$) is determined
from the position of the threshold crossing, fit to the shape of the signal and measured value of the pedestal. 

In the hit reconstruction stage, individual signals in terms of the ADC channels is converted into the number of
the photoelectrons ($nphe_{hi}$) for each hit using the gain constants in the $ccdb$: 

\begin{equation}
nphe_{hi} = \frac{ADC}{gain}
\end{equation}

The correspondence between the  hit coordinate in terms (sector, layer, component) and specific PMT, observing
the mirror,  provides the information on the angular hit coordinates ($\theta_{hi}$, $\phi_{hi}$).

In order to reconstruct the real signal strength ($nphe_c$), we need to combine split signals (hits)into the single
cluster. We start by selecting the strongest hit and use it as a starting point of the cluster. Then we look for the
adjacent hits within a certain time window.  If such hits are found, they will be added to the growing cluster. If
not, the cluster growth stops.

The total signal strength is determined as the sum of the individual signals, and the signal time is determined as
the average between the individual signals, weighted by the corresponding number of the photoelectrons. The hit
angular coordinate is determined as the average between the individual hits forming the cluster.  Hits, attributed
to the established clusters, are removed from the further consideration, and algorithm starts to look for the next
cluster until list of hits is exhausted.

\begin{eqnarray*}
nphe_c &= \frac{\sum_{i=1}^{N}{nphe_{hi}}}{N}\\
T_c &= \frac{\sum_{i=1}^{N}{N*T_{hi}}}{\sum_{i=1}^{N}{nphe_{hi}}}\\	
\theta_c &=\frac{\sum_{i=1}^{N}{\theta_{hi}}}{N}\\
\phi_c &= \frac{\sum_{i=1}^{N}{\phi_{hi}}}{N}
\end{eqnarray*}

Hits, attributed to the established clusters, are removed from the further consideration, and algorithm starts to
look for the next cluster until list of hits is exhausted. 
	 
Cluster coordinate, required for the matching of the hit with the reconstructed track in the Event Builder, is
reconstructed by projecting  ($\theta$, $\phi$) of the cluster on the surface of the ellipsoidal mirror.		

\subsubsection{Ring-Imaging Cherenkov Counter}

\subsubsection{Low Threshold Cherenkov Counter}

\subsection{Time-of-Flight Systems}

For the CLAS12 Forward Time-of-Flight system (FTOF)~\cite{ftof-nim} and Central Time-of-Flight system
(CTOF)~\cite{ctof-nim}, the raw data from the detector elements readout during data acquisition that are
associated with a charged or neutral hit, include an ADC charge and hit time from a fitted flash ADC waveform
and a TDC time. The ADC and TDC information is recorded only for hits that are above the hardware readout
threshold. For hits in the FTOF and CTOF, the hardware thresholds for the flash ADC and discriminators are set
to be $\sim$1~MeV. During event reconstruction the ADC and TDC information are converted into deposited energy
and timing information after completion of the detector calibration procedures (see Refs.~\cite{ftof-nim,ctof-nim}
for calibration details). The measured hit times in each counter are related to the event start time defined with
respect to the appropriate RF beam bucket to determine the charged or neutral particle flight time.

\subsubsection{Reconstructed Hit Time}
\label{rec:time}

The reconstructed scintillation bar hit times need to account for the time delays along the readout path that
include the PMT signal transit time, the signal propagation times through the signal cables and the electronics, 
and any time-walk effects associated with the readout discriminators. For the FTOF readout, leading-edge
discriminators are employed, while for the CTOF readout constant fraction discriminators are employed and no
external time-walk corrections are required. The hit times reconstructed by the readout through the PMTs at
the left and right ends of each scintillation bar are given by:

\begin{equation}
t_{L/R} = (C_{TDC} \cdot TDC_{L/R}) - t_{L/R}^{walk} - \frac{C_{LR}}{2} + C_{p2p},
\end{equation}

\noindent
where $C_{TDC}$ is the TDC channel-to-time conversion factor (0.024~ns/bin), $TDC$ is the measured TDC value
relative to the trigger signal, $t^{walk}$ is the time walk function to correct for the pulse amplitude dependence of
the crossing times of the discriminator threshold, $C_{LR}$ is a time offset to center the left-right TDC difference
distribution about 0, and $C_{p2p}$ is a time offset to account for the different delay times along the signal path from
the photocathode of the PMT to the readout electronics. The paddle-to-paddle time offsets $C_{p2p}$ mainly account
for the different signal cable lengths.

The FTOF and CTOF particle hit times relative to the trigger signal can be determined separately from the times
$t_L$ and $t_R$ measured by the left and right PMTs of a given scintillation bar using:

\begin{equation}
t_{hit}^{L/R} = t_{L/R} - \frac{d_{L/R}}{v_{eff}},
\end{equation}

\noindent
where $d_{L/R}$ represents the distances along the bar from the hit point to the left and right PMTs given by
$d_{L/R}= L/2 \pm y$ with $y$ the hit coordinate along the bar and $L$ the counter length. The average hit time
is given by:

\begin{equation}
\bar{t}_{hit} = \frac{1}{2} ( t_{hit}^L + t_{hit}^R ) = \frac{1}{2} \left[ t_L + t_R - \frac{L}{v_{eff}} \right],
\end{equation}

\noindent
where $v_{eff}$ is the effective speed of light in the scintillation bar.

The hit coordinate along the bar is defined with respect to the center of the bar using:

\begin{equation}
  \label{tof-coor}
  y = \frac{v_{eff}}{2} (t_L - t_R - y_{offset}),
\end{equation}

\noindent
where $y_{offset}$ is an additional coordinate shift to center the coordinate distribution about zero.

In addition to the hit time determined from the TDC information associated with the left and right PMTs
from each counter of the FTOF and CTOF systems, a time is also derived from fitting the leading edge of
the flash ADC pulse shape. Due to the choice of fast timing PMTs for the detector readout and the use of
250~MHz FADCs, the number of samples on the leading edge of the PMT pulses is only 3 to 4, hence
the timing resolution is only $\sim$1~ns. In the event reconstruction of FTOF and CTOF hits, however, we
require that the hit times determined from the TDCs and from the FADCs match to within 10~ns to
reduce the probability of a mismatch of the ADC and TDC data for a given scintillation bar hit.

For CLAS12 flight time measurements, the event start time is determined by the FTOF system using the
path length $L$ determined from the forward drift chamber tracking by comparing the FTOF hit time
traced back to the vertex and linking this time to the closest RF pulse from the 499~MHz RF signal from
the accelerator. The timing residuals

\begin{equation}
  t_{res} = mod \left [ \left (\bar{t}_{hit} - \frac{L}{\beta c} \right) - \left (t_{RF} + \frac{z_v}{\beta_e c} \right),
    T_{RF} \right ]
\end{equation}

\noindent
are centered about zero. The reconstruction algorithm determines the event start time looking first for an
$e^-$ and then an $e^+$ in the ECAL. If none exists, then it looks for a high momentum $\pi^+$ or $\pi^-$. In
each case, if multiple candidates exist, the track with the highest momentum is chosen. In the expression for
$t_{res}$ above, $z_v$ represents the reconstructed event vertex from traceback of the drift chamber track
to the distance of closest approach to the defined beamline and $T_{RF}$ is the RF period. The term
$z_v/\beta_e c$ shifts the RF reference time to the event vertex. The modulus is employed as the RF time is a
pulse train with a period of $T_{RF}$. For the CTOF reconstruction, the flight time is computed as
$\bar{t}_{hit} - t_{ST}$.

\subsubsection{Reconstructed Hit Energy}
\label{rec:energy}

The reconstructed energy from the left and right ADC values of the PMTs for a given scintillator bar is given by:

\begin{equation}
E_{L/R} = (ADC_{L/R} - PED_{L/R}) \left [ \frac{\left( \frac{dE}{dx} \right )_{MIP} \cdot t}{ADC_{MIP}} \right ],
\end{equation}

\noindent
where $(ADC  - PED)$ is the measured pedestal-subtracted ADC integral, $ADC_{MIP}$ is the ADC value for normally
incident minimum-ionizing particles (MIPs) at the center of the scintillation bar, $\left( \frac{dE}{dx} \right)_{MIP}$
is the energy loss for MIPs in the scintillation bars (1.956~MeV/cm), and $t$ is the scintillation bar thickness.
The deposited energy is computed as the geometric mean of the deposited energy as determined from the left and
right PMTs $E_L$ and $E_R$ as $E_{dep} = \sqrt{E_L E_R}$.

\subsubsection{Hit Clustering and Matching}

A charged track incident upon the FTOF or CTOF can cross more than one scintillation bar. If there are multiple
scintillation bar hits associated with a single incident charged particle track, a hit cluster can be defined. These
clusters have associated with them a hit coordinate, energy, and hit time.

The $(x,y,z)$ coordinates assigned to a scintillation bar hit are given at the middle of the bar across its width
and depth, and from eq.(\ref{tof-coor}) along the counter length. Hits are assigned as part of a cluster if they
fall within a matching distance of the charged track projected from the drift chambers to the FTOF counters
using a straight line extrapolation or from the Central Vertex Tracker to the CTOF counters following the
helical trajectory in the solenoid field. This matching distance is $\pm$1 counter about the hit scintillation bar
and $\pm$10~cm along the counter length.

With hit clusters defined, the associated cluster coordinate is defined as the energy-deposited weighted average.
The deposited energy of the cluster hit is the energy sum of the cluster hits and the hit time is the energy-deposited
weighted average. Note that in both the FTOF and CTOF systems, the maximum cluster size is $N=2$.

For the FTOF system in the range of polar angles from 5$^\circ$ to 30$^\circ$ where two parallel scintillation
bar planes are included (referred to as panel-1b - closest to the target, and panel-1a - farthest from the target)
(see Ref~\cite{ftof-nim}), the hit time is assigned as the time resolution weighted average of the two cluster times
after evolving the panel-1a hit to the location of the panel-1b hit using the path length determined from the
extrapolated drift chamber track. The combined plane hit time is given by:

\begin{equation}
t_{comb} = \dfrac{ t_{1b}^{cluster} {\delta_{1b}}^{-1} + (t_{1a}^{cluster} - \Delta r/\beta) {\delta_{1a}}^{-1}}
           {\left( {\delta_{1b}}^{-1} + {\delta_{1a}}^{-1} \right)},
\end{equation}

\noindent
where $\delta_{1b/1a}$ are the measured counter effective time resolutions, $t_{1b/1a}^{cluster}$ are the cluster
hit times, and $\Delta r/\beta$ is the path length from the panel-1a hit location to the panel-1b hit location for
the track of speed $\beta$.

As the effective FTOF time resolutions for the panel-1b counters are in the range from $60-110$~ps and those
for the panel-1a counters are in the range from $90-160$~ps, the combined plane hit time resolution is $\sim$20\%
better than that for the panel-1b hit time resolution. If there is a panel-1b (panel-1a) hit without an associated
panel-1a (panel-1b) hit, the hit time is defined solely by the panel-1b (panel-1a) hit.

\subsection{Central Neutron Detector}

The reconstruction of the Central Neutron Detector (CND) is done in three steps:

\begin{itemize}
\item{the reconstruction of the time and position of the hit in the paddle;}
\item{the reconstruction of the deposited energy;}
\item{the matching of CND hits with tracks coming from the interaction vertex.}
\end{itemize}

The reconstruction makes use of the calibration constants of the CND summarized in
Table~\ref{table_cnd_constants}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|} \hline
Constant Name & Number of Constants  & Units \\ \hline
$t_{\rm{LR}}$ & 72 & ns\\ \hline
$v_{\rm{eff}}$ & 144 & cm/ns \\ \hline
$u_{\rm{t}}$ & 72 & ns \\ \hline
$t_{\rm{LR}_{\rm{ad}}}$ & 72 & ns \\ \hline
$t_{\rm{off}}$ &72 & ns\\ \hline
$A_{\rm{L}}$ & 144 & cm\\ \hline
$MIP_{\rm{D}}$, $MIP_{\rm{I}}$ & 144 each & no units \\ \hline
\end{tabular}
\caption{The constants computed in the CND calibration.}
\label{table_cnd_constants}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Timing Calibration}

The paddle in which the hit occurs must be determined before all other reconstruction steps. The raw hit
times are obtained from the measured TDC channel using a slope constant of 0.0234~ns/channel for all
channels.

The left and right times of a hit in the left paddle (we label them as $t_{{\rm{L}}/{\rm{L}}}$ and
$t_{\rm{R}/{\rm{L}}}$ where the first index corresponds to the paddle under exam, while the second indicates
the paddle in which the primary hit happened) are given by:

\begin{equation}
\label{eq_time_hit_lr}
t_{{\rm{L}}/{\rm{L}}}=t_{\rm{off}}+t_{\rm{tof}}+\frac{z}{v_{\rm{eff}_L}}+t_{\rm{S}}+t_{\rm{off}_{\rm{L}}}+{\rm{TDC}}_{\rm{j}},
\end{equation}

\begin{equation}
\label{eq_time_hit_lr1}
t_{{\rm{R}/{\rm{L}}}}=t_{\rm{off}}+t_{\rm{tof}}-\frac{z}{v_{\rm{eff}_L}}+\frac{L}{v_{\rm{eff}_L}}
+\frac{L}{v_{\rm{eff}_{\rm{R}}}}+u_{\rm{t}}+t_{\rm{S}}+t_{\rm{off}_{\rm{R}}}+{\rm{TDC}}_{\rm{j}},
\end{equation}

\noindent
where $t_{\rm{tof}}$ is the time of flight, $z$ is the position of the hit measured from the upstream end of the
paddle, $L$ is the length of the paddle, $t_{\rm{S}}$ is the start time of the event, $t_{\rm{off}_{\rm{L}}}$ and
$t_{\rm{off}_{\rm{R}}}$ are time offsets associated to the left and right coupled paddles, and ${\rm{TDC}}_{\rm{j}}$
is the TDC clock jitter. Similarly if the hit happened in the right paddle one can write:

\begin{equation}
  t_{{\rm{L}}/{\rm{R}}}=t_{\rm{off}}+t_{\rm{tof}}-\frac{z}{v_{\rm{eff}_{\rm{R}}}}+\frac{L}{v_{\rm{eff}_L}}
  +\frac{L}{v_{\rm{eff}_{\rm{R}}}}+u_{\rm{t}}+t_{\rm{S}}+t_{\rm{off}_{\rm{L}}}+{\rm{TDC}}_{\rm{j}},
\end{equation}

\begin{equation}
t_{R/R}=t_{\rm{off}}+t_{\rm{tof}}+\frac{z}{v_{\rm{eff}_{\rm{R}}}}+t_{\rm{S}}+t_{\rm{off}_{\rm{R}}}+{\rm{TDC}}_{\rm{j}}.
\end{equation}

Defining $\Delta$ and $\Delta'$ as:

\begin{equation}
\Delta=\frac{L}{v_{\rm{eff}_L}}-\frac{L}{v_{\rm{eff}_{\rm{R}}}},
\end{equation}

\begin{equation}
\Delta'=t_{\rm{L}/{\rm{X}}}-t_{\rm{R}/{\rm{X}}}+t_{\rm{off}_{\rm{R}}}-t_{\rm{off}_{\rm{L}}},
\end{equation}

\noindent
where the index X can be R or L, one can compute $\Delta-\Delta'$ for both cases (hit in the left paddle or hit
in the right paddle). If the hit is in the left paddle:

\begin{equation}
\Delta'-\Delta= \frac{2z}{v_{\rm{eff}_L}} - \frac{2L}{v_{\rm{eff}_L}} -u_{\rm{t}} <0.
\end{equation}

\noindent
If the hit is in the right paddle:

\begin{equation}
\Delta'-\Delta= \frac{2L}{v_{\rm{eff}_{\rm{R}}}}-\frac{2z}{v_{\rm{eff}_{\rm{R}}}} +u_{\rm{t}} >0.
\end{equation}

\noindent
If $\Delta'<\Delta$, the paddle in which the hit happened is the left one, otherwise it is the right one.

\subsubsection{Hit Position and Time Reconstrution}

The reconstruction of the time and position of a hit will be shown for the case of a hit in the left paddle. In
case of a hit in the right paddle the applicable equations are obtained by switching the $L/R$ indices.

Starting from $t_{\rm{L}}$ and $t_{\rm{R}}$, defined in Eq.~\ref{eq_time_hit_lr} and Eq.~\ref{eq_time_hit_lr1}
and substracting the time offsets obtained from the calibrations, the start time and the time jitter, one can
define the propagation times $t_{\rm{L}_{\rm{prop}}}$ and $t_{\rm{R}_{\rm{prop}}}$ as:

\begin{equation}
t_{\rm{L}_{\rm{prop}}}=t_{\rm{tof}}+\frac{z}{v_{\rm{eff}_{\rm{L}}}},
\end{equation}

\begin{equation}
t_{\rm{R}_{\rm{prop}}}=t_{\rm{tof}}-\frac{z}{v_{\rm{eff}_{\rm{L}}}}+\frac{L}{v_{\rm{eff}_{\rm{L}}}}
+\frac{L}{v_{\rm{eff}_{\rm{R}}}}+u_{\rm{t}}.
\end{equation}

The position of the hit is then obtained as the difference of the propagation times:

\begin{equation}
z=\frac{v_{\rm{eff}_{\rm{L}}}}{2} \left(t_{\rm{L}_{\rm{prop}}}-t_{\rm{R}_{\rm{prop}}}
+ L \cdot \left(\frac{1}{v_{\rm{eff}_{\rm{L}}}}+\frac{1}{v_{\rm{eff}_{\rm{R}}}}\right)  +u_{\rm{t}}\right).
\end{equation}

The $x$ and $y$ coordinates of the hit are obtained from the radius and the azimuthal angle of the hit, which
are, in turn, determined by knowing the layer, sector, and component (left or right) of the hit.  Finally, the time
of flight of the particle that produced the hit is obtained as:

\begin{equation}
t_{\rm{tof}}= \frac{1}{2}\left(t_{\rm{L}_{\rm{prop}}}+t_{\rm{R}_{\rm{prop}}}- L \cdot \left(\frac{1}{v_{\rm{eff}_{\rm{L}}}}
+\frac{1}{v_{\rm{eff}_{\rm{R}}}}\right)  -u_{\rm{t}}\right).
\end{equation}

\subsubsection{Energy Reconstruction}

For hits in the left paddle, the two associated ADCs can be written as:

\begin{equation}
\label{eq_rec_3}
ADC_{\rm{L}}=\frac{E_{\rm{L}}}{E_0}\cdot MIP_{\rm{D}}\cdot e^{\frac{-z}{A_{\rm{L}}}},
\end{equation}

\begin{equation}
\label{eq_rec_4}
ADC_{\rm{R}}=\frac{E_{\rm{R}}}{E_0}\cdot MIP_{\rm{I}}\cdot e^{\frac{-(L-z)}{A_{\rm{L}}}},
\end{equation}

\noindent
where $E_{L/R}$ is half the energy deposited by the particle in the left/right paddle and $E_0$ is given by
Eq.~\ref{eq_def_e0}.

\begin{equation}\label{eq_def_e0}
E_0=\frac{h\cdot 1.956}{2}~\rm{MeV},
\end{equation}

\noindent
where $h$ is the thickness of each scintillator. The above equations are valid for hits in the left paddles, while
for hits in the right paddles, the applicable equations are obtained by switching the $L/R$ indices. From
Eqs.~\ref{eq_rec_3} and \ref{eq_rec_4} follows the relations:

\begin{equation}
E_{\rm{L}}=\frac{ADC_{\rm{L}} \cdot E_0}{MIP_{\rm{D}}}\cdot e^{\frac{z}{A_{\rm{L}}}},
\end{equation}

\begin{equation}
E_{\rm{R}}=\frac{ADC_{\rm{R}} \cdot E_0}{MIP_{\rm{I}}}\cdot e^{\frac{L-z}{A_{\rm{R}}}}.
\end{equation}

\noindent
The total deposited energy is given by the sum of $E_{\rm{L}}$ and $E_{\rm{R}}$:

\begin{equation}
E_{\rm{dep}}=E_{\rm{L}}+E_{\rm{R}}.
\end{equation}

\subsubsection{Hit/Track Matching}

Tracks from charged particles crossing the CLAS12 Central Vertex Tracker (CVT) are associated to hits in
the CND. This allows, for each CND hit matched with a CVT track, to calculate the position of the hit from the
extrapolated track, the pathlength between the track vertex and the hit and the path travelled in the hit
paddle. This information is used in the calibration, as well as to veto charged particles when looking for neutrons
in the CND. CVT tracks are extrapolated to radii corresponding to the entry point, middle point and exit point of
the track in the paddle. These points are defined as the intersections between the helix of the track and cylinders
of radii corresponding to the distances between the center of the CD and the three CND layers. A CVT track and
a CND hit are matched if the hit coordinates ($x$, $y$, and $z$) and the extrapolated coordinates ($x_m$,
$y_m$, and $z_m$) verify the relations:

\vfil
\eject

\begin{equation}
\mid x-x_m \mid < \sigma_x ,~~~~\mid y-y_m \mid < \sigma_y , ~~~~z_m  \in [-\sigma_z,L+\sigma_z],
\end{equation}

\noindent

where $\sigma_z=1.5$~cm, $ L$ is the length of a paddle, and $\sigma_x$ and $\sigma_y$ are given by:

\begin{equation}
\sigma_x= \sqrt{x^2\frac{\sigma_R^2}{R^2}+y^2\sigma_\phi^2},~~~~
\sigma_y= \sqrt{y^2\frac{\sigma_R^2}{R^2}+x^2\sigma_\phi^2},
\end{equation}

\noindent
where $R$ is the radius of the hit, $\sigma_R$ is half the thickness of a paddle (1.5~cm) and $\sigma_\phi$
is the azimuthal resolution of each paddle (3.75$^\circ$). The path travelled by the particle in the paddle is
approximated as the distance between the entry and exit points. The path length between the vertex and the
hit is given by the helix parameters.

\subsection{Forward Tagger}
~~

\subsection{Event Builder}
~~

\subsubsection{Particle Identification Performance}
~~

\section{Data Processing}
~~

\subsection{Data Processing Workflow}
~~

\subsection{DSTs and Analysis Trains}
~~

\subsection{Computing Tools}
~~

\subsection{Resources}
~~

\section{Code Management}
~~

\subsection{Repositories}
~~

\subsection{Releases}
~~

\subsection{Unit Tests}
~~

\subsection{Code Validation Suites}
~~

\section{Conclusions}
~~

\ack

This work was supported in part by DOE Contract DE-AC05-84ER40150.

\begin{thebibliography}{99}

\bibitem{clara-2011}
V. Gyurjyan {\it et al.}, CLARA: A Contemporary Approach to Physics Data Processing, J. Phys. Conf. Ser.
{\bf 331}, 032013 (2011).

\bibitem{clara-service}
J. Carbonneau, M. Moog,  J. Gilfoyle {\it et al.}, 2011 APS Division of Nuclear Physics Meeting Abstracts, EA.024.

\bibitem{framework}
  Component Based Dataflow Processing Framework, 2015, IEEE DOI: 10.1109/BigData.2015.7363971,
  ISBN: 978 1-4799-9926-2

\bibitem{clara-2016}
CLARA: The CLAS12 Reconstruction and Analysis framework, 2016, J. Phys. Conf. Ser. {\bf 762}, 012009 (2016).

\bibitem{clas12-nim}
V.D. Burkert {\it et al.}, ``The CLAS12 Spectrometer at Jefferson Laboratory'', to be published in Nucl.
Inst. and Meth. A, (2020). (see this issue)

\bibitem{ftof-nim}
D.S. Carman {\it et al.},   ``The CLAS12 Forward Time-of-Flight System'', to be published in Nucl.
Inst. and Meth. A, (2020). (see this issue)

\bibitem{ctof-nim}
D.S. Carman {\it et al.}, ``The CLAS12 Central Time-of-Flight System'', to be published in Nucl.
Inst. and Meth. A, (2020). (see this issue)

\bibitem{ecal-nim}
G. Asryan {\it et al.}, ``The CLAS12 Forward Electromagnetic Calorimeter'', to be published in Nucl.
Inst. and Meth. A, (2020). (see this issue)

\bibitem{dc-nim}
M.D. Mestayer {\it et al.}. ``The CLAS12 Drift Chamber System'', to be published in Nucl. Inst. and
Meth. A, (2020). (see this issue)

\bibitem{ft-nim}
M. Battaglieri {\it et al.}, ``The CLAS12 Forward Tagger System'', to be published in Nucl. Inst. and
Meth. A, (2020). (see this issue)

\bibitem{spiri}
Alexander Spiridonov, Optimized Integration of the Equations of Motion of a Particle in the HERA-B Magnet

\bibitem{CA-HeraB}
  I.~Abt, I.~Kisel, S.~Masciocchi and D.~Emelyanov,
  ``CATS: A cellular automaton for tracking in silicon for the HERA-B vertex detector,''
  Nucl.\ Instrum.\ Meth.\ A {\bf 489} (2002) 389.
  doi:10.1016/S0168-9002(02)00790-8
  %%CITATION = doi:10.1016/S0168-9002(02)00790-8;%%
  %26 citations counted in INSPIRE as of 27 Sep 2019

\bibitem{jlab-batch-farm}
    Jefferson Lab Batch Farm,
    https://scicomp.jlab.org/docs/ExpPhyComp

\bibitem{jlab-tape-silo}
    Jefferson Lab Tape Silo,
    https://scicomp.jlab.org/docs/node/9

\end{thebibliography}

\end{document}
